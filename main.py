from ECDICT.stardict import StarDict
from typing import List
from collections import defaultdict
from pdfminer.high_level import extract_text
import re
from loguru import logger
from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from tabulate import tabulate
import glob
try:
    import ipdb
except Exception:
    pass


class Words():
    def __init__(self, filenames:List[str]) -> None:
        # init db
        db = "./ECDICT/ecdict.db"
        self.sd = StarDict(db, False)
        print(f"{db} ç›®å‰å·²æ”¶å½•äº† {self.sd.count()} ä¸ªå•è¯")
        
        # pdf2words
        self._words_cnt = defaultdict(int)
        for fn in filenames:
            assert fn.endswith(".pdf")
            self.pdf2words(fn)

    def pdf2words(self, fn):
        """æå–pdfé‡Œçš„å•è¯"""
        # pdfè½¬åŒ–ä¸ºæ–‡æœ¬
        text = extract_text(fn)

        # é¢„å¤„ç†
        text = text.lower()
        text = text.replace("\n\n", "\n").replace("-\n", "").replace("\n", " ")
        text = " " + text + " "
        
        # å»é™¤å¥‡æ€ªçš„å­—ç¬¦
        text = text.replace("ï¬", "fi")
        text = text.replace("Ã—", "x")
        text = text.replace("ï¬‚", "fl")
        s = set(
            "qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM0123456789" 
            ",.()[]{}<>?!@/\\+-~\"\' :\*%;=â€˜â€™`_|â€â€œÂ·Â¨Â´"
            "âˆ—â€ "
            "ğŸ‘â€“ğ‘ğŸâ€¡â€¢"
            "\x0c"
            "Î‘Î±Î’Î²Î“Î³Î”Î´Î•ÎµÏµÎ–Î¶Î—Î·Î˜Î¸Î™Î¹ÎšÎºÎ›Î»ÎœÎ¼ÂµÎÎ½ÎÎ¾ÎŸÎ¿Î Ï€Î¡ÏÎ£ÏƒÏ‚Î¤Ï„Î¥Ï…Î¦Ï†Ï•Î§Ï‡Î¨ÏˆÎ©Ï‰"
        )
        unsupported = set(filter(lambda x:x not in s,text))
        if len(unsupported) > 0:
            logger.warning(f"æœ‰äº›å­—ç¬¦å¯èƒ½ä¸æ”¯æŒ, éœ€è¦æ‰‹åŠ¨æ·»åŠ è§„åˆ™: {list(unsupported)}")

        # åªä¿ç•™çš„è‹±è¯­å•è¯(æœ€å¤šå¸¦ç€-)
        # wanted, unwanted = "[a-zA-Z\-]", "[^a-z^A-Z^\-]"
        wanted, unwanted = "[a-zA-Z]", "[^a-z^A-Z]"
        words = re.findall(f"{unwanted}({wanted}+){unwanted}", text)
        
        # å»é™¤å¤ªçŸ­çš„è‹±è¯­å•è¯
        words = list(filter(lambda x:len(x)>4, words))
               
        # è¿˜åŸå•è¯è¯æ€§
        words = [self.Lemmatization(word) for word in words]
        
        # å»é‡, ç»Ÿè®¡é¢‘ç‡
        for word in words:
            self._words_cnt[word] += 1

    def Lemmatization(self, word):
        if not hasattr(self, "wnl"):
            self.wnl = WordNetLemmatizer()
        def get_wordnet_pos(tag):
            if tag.startswith('J'):
                return wordnet.ADJ
            elif tag.startswith('V'):
                return wordnet.VERB
            elif tag.startswith('N'):
                return wordnet.NOUN
            elif tag.startswith('R'):
                return wordnet.ADV
            else:
                return None
        sentence = word
        tokens = word_tokenize(sentence)  # åˆ†è¯
        tagged_sent = pos_tag(tokens)     # è·å–å•è¯è¯æ€§
        wnl = WordNetLemmatizer()
        lemmas_sent = []
        for tag in tagged_sent:
            wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN
            lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # è¯å½¢è¿˜åŸ
        return lemmas_sent[0]
        
    def translate(self):
        self._words_translate = dict()
        for word in list(self._words_cnt.keys()):
            data = self.sd.query(word)
            if data is None:
                self._words_cnt.pop(word)
                logger.warning(f"{word}æ— æ³•ç¿»è¯‘ï¼Œ æ•°æ®åº“ä¸­ä¸å­˜åœ¨è¯¥å•è¯")    
            elif (   'äººå' in data['translation'] 
                  or 'ç”·å­å' in data['translation'] 
                  or 'ç”·å' in data['translation'] 
                  or 'å¥³å­å' in data['translation']
                  or 'å¥³å' in data['translation']
                  or 'å§“æ°' in data['translation'] 
                  or 'åœ°å' in data['translation'] 
                  ):# å»é™¤äººåã€åœ°å
                
                self._words_cnt.pop(word)
                logger.warning(f"{word} å»é™¤äººå")    
            # elif data['collins'] is None:
            #     self._words_cnt.pop(word)
            #     logger.warning(f"{word} collinsä¸å­˜åœ¨, å¯èƒ½æ˜¯æŸä¸ªå•è¯çš„å˜ä½“")
            else:
                self._words_translate[word] = data
        
        self.sorted_words = sorted(self._words_cnt.keys(), key=lambda x:self._words_cnt[x], reverse=True)

    def get_hard(self, hard_frq=500, fmt="csv", filename="./outputs/tmp.csv", my_easy_words=None):
        if not hasattr(self, "old_easy_words"):
            self.old_easy_words = []
            for csv in glob.glob("./easy_words/*.csv"):
                with open(csv, encoding="utf-8") as f:
                    self.old_easy_words += [i.split(',')[0] for i in f.readlines()]
            self.old_easy_words = set(self.old_easy_words)

        self.hard_words = self.sorted_words
        self.hard_words = list(filter(
            lambda x: not 0<self._words_translate[x]['frq']<hard_frq, 
            self.hard_words
        ))
        self.hard_words = list(filter(
            lambda x: x not in self.old_easy_words, 
            self.hard_words
        ))
        res = []
        for word in self.hard_words:
            s = self._words_translate[word]['translation'].replace('\n',' ')
            res.append([word,s])
            # print(f"    é¢‘ç‡: {self._words_cnt[word]}")
            # print(f"    æŸ¯æ—æ–¯æ˜Ÿçº§: {self._words_translate[word]['collins']}")
            # print(f"    å½“ä»£è¯­æ–™åº“è¯é¢‘é¡ºåº: {self._words_translate[word]['frq']}")
        logger.info(f"get_hard Done, got {len(res)} hard words")
        if "print" in fmt:
            print(tabulate(res, ["words", "translation"], "grid",maxcolwidths=[30, 60]))
        if "csv" in fmt:
            with open(filename, "a", encoding="gbk") as f:
                print("\n".join([i[0]+","+i[1].replace(',',";") for i in res]), file=f)
            with open(filename.replace(".csv", "-utf-8.csv"), "a", encoding="utf-8") as f:
                print("\n".join([i[0]+","+i[1].replace(',',";") for i in res]), file=f)
        if "return" in fmt:
            return res
        
def main():
    # init words
    words = Words(["inputs/YOLOV6-2209.02976.pdf"])
    words.translate()
    words.get_hard(500,"csv", "./outputs/yolov6.csv")
    
if __name__ == "__main__":
    main()